{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Learning Cache Replacement Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load extract_tar_gz.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "EXTENSION = '.tar.gz'\n",
    "BLKPARSE = '.blkparse'\n",
    "\n",
    "\n",
    "def main(input_dir, output_dir):\n",
    "  assert os.path.isdir(input_dir), \"The input directory {} does not exist\".format(input_dir)\n",
    "\n",
    "  input_dir = input_dir.rstrip(\"/\")\n",
    "  if not os.path.isdir(output_dir):\n",
    "    print(\"Creating output directory {}\".format(output_dir))\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "  for file in os.listdir(input_dir):\n",
    "    if not file.endswith(EXTENSION):\n",
    "      continue\n",
    "    \n",
    "    print(\"Extracting and moving {}\".format(file))\n",
    "    execute_command(\"tar -xvzf {}\".format(input_dir + '/' + file))\n",
    "    execute_command(\"mv ./*{} {}\".format(BLKPARSE, output_dir))\n",
    "    \n",
    "\n",
    "def execute_command(command):\n",
    "  os.system(command)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--input_dir\", type=str, help=\"the directory containing the tar.gz files\", required=True)\n",
    "  parser.add_argument(\"--output_dir\", type=str, help=\"the directory containing content of the tar.gz files\", required=True)\n",
    "  args = parser.parse_args() \n",
    "  main(args.input_dir, args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "from collections import Counter, deque, defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompress tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"FIU_raw\"\n",
    "output_dir = \"FIU_trace\"\n",
    "main(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Cache Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum block number\n",
    "maxpos = 1000000000000\n",
    "\n",
    "# Number of features (Recency, Frequency, Block No.)\n",
    "num_params = 3\n",
    "\n",
    "# Cache Size\n",
    "cache_size = 100\n",
    "\n",
    "# Sequence Length\n",
    "sequence_length = 5\n",
    "\n",
    "\n",
    "sampling_freq = cache_size\n",
    "\n",
    "# How many % of cache to use\n",
    "eviction = int(0.7 * cache_size)  \n",
    "\n",
    "# Results\n",
    "lruCorrect = 0\n",
    "lruIncorrect = 0\n",
    "\n",
    "lfuCorrect = 0\n",
    "lfuIncorrect = 0\n",
    "\n",
    "# Variables\n",
    "X = np.array([], dtype=np.int64).reshape(0,num_params)\n",
    "Y = np.array([], dtype=np.int64).reshape(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load workload\n",
    "**cheetah.cs.fiu.edu-110108-113008.1.blkparse** does not contain correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2271127"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = \"FIU_trace/cheetah.cs.fiu.edu-110108-113008.2.blkparse\"\n",
    "\n",
    "df = pd.read_csv(train, sep=' ',header = None)\n",
    "df.columns = ['timestamp','pid','pname','blockNo', \\\n",
    "              'blockSize', 'readOrWrite', 'bdMajor', 'bdMinor', 'hash']\n",
    "\n",
    "trainBlockTrace = df['blockNo'].tolist()\n",
    "trainBlockTrace = trainBlockTrace[:int(len(trainBlockTrace)*0.1)]\n",
    "\n",
    "len(trainBlockTrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert workload file into sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Shehbaz\n",
    "def get_recency(lru, cache):\n",
    "    recency = []\n",
    "    recency_dict = defaultdict(int)\n",
    "    \n",
    "    # Compute the recency order of each page in cache\n",
    "    for time in range(len(lru)):\n",
    "        recency_dict[lru[time]] = time\n",
    "        \n",
    "    for block in cache:\n",
    "        recency.append(recency_dict[block])\n",
    "\n",
    "    return recency\n",
    "\n",
    "def get_frequency(lfu, cache):\n",
    "    frequency = []\n",
    "    \n",
    "    for block in cache:\n",
    "        frequency.append(lfu[block])\n",
    "    return frequency\n",
    "\n",
    "def normalize_columns(input):\n",
    "    return normalize(input, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_length_input(lfu, lru, cache, preprocess_func):\n",
    "    input_recency = get_recency(lru, cache)\n",
    "    input_frequency = get_frequency(lfu, cache)\n",
    "    input_block_num = cache[:]\n",
    "    \n",
    "    # Columns: recency, frequency, block number\n",
    "    # Row: cache location\n",
    "    raw_input = np.column_stack((input_recency, input_frequency, input_block_num))\n",
    "    \n",
    "    return preprocess_func(raw_input)\n",
    "\n",
    "\n",
    "SEQ_DIM = 0\n",
    "def get_multiple_length_input(sequence_length, prev_inputs, lfu, lru, cache, preprocess_func):\n",
    "    assert prev_inputs.shape[SEQ_DIM] == sequence_length\n",
    "    current_input = get_single_length_input(lfu, lru, cache, preprocess_func)\n",
    "    return np.vstack((prev_inputs[1:], current_input[None]))\n",
    "    \n",
    "\n",
    "def get_output(pre_cache, post_cache):\n",
    "    pass\n",
    "\n",
    "# Taken from Shehbaz.\n",
    "def getY(C,D):\n",
    "    assert(len(C) == len(D))\n",
    "    Y_current = []\n",
    "    KV_sorted = Counter(D)\n",
    "    evict_dict = dict(KV_sorted.most_common(eviction))\n",
    "    assert(len(evict_dict) == eviction)\n",
    "    all_vals = evict_dict.values()\n",
    "    for e in C:\n",
    "        if e in evict_dict.values():\n",
    "            Y_current.append(1)\n",
    "        else:\n",
    "            Y_current.append(0)\n",
    "    #print (Y_current.count(1))\n",
    "    assert(Y_current.count(1) == eviction)\n",
    "    assert((set(all_vals)).issubset(set(C)))\n",
    "    return Y_current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belady Optimal Algorithm (From Shehbaz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belady_opt(blocktrace, frame):\n",
    "    global maxpos\n",
    "    \n",
    "    optimal = defaultdict(deque)\n",
    "    deleted = defaultdict(int)\n",
    "    lfu = defaultdict(int)\n",
    "    lru = []\n",
    "\n",
    "    for i, block in enumerate(tqdm(blocktrace, desc=\"OPT: building index\")):\n",
    "        optimal[block].append(i)\n",
    "\n",
    "    hit, miss = 0, 0\n",
    "\n",
    "    C = []\n",
    "    for seq_number, block in enumerate(tqdm(blocktrace, desc=\"OPT\")):\n",
    "\n",
    "        LFUDict[block] +=1\n",
    "\n",
    "        if len(OPT[block]) is not 0 and OPT[block][0] == seq_number:\n",
    "            OPT[block].popleft()\n",
    "\n",
    "            \n",
    "        if block in C:\n",
    "            hit+=1\n",
    "            LRUQ.remove(block)\n",
    "            LRUQ.append(block)\n",
    "            assert( seq_number in D)\n",
    "            del D[seq_number]\n",
    "            if len(OPT[block]) is not 0:\n",
    "                D[OPT[block][0]] = block\n",
    "                OPT[block].popleft()\n",
    "            else:\n",
    "                D[maxpos] = block\n",
    "                maxpos -= 1\n",
    "        else:\n",
    "            miss+=1\n",
    "            if len(C) == frame:\n",
    "                assert(len(D) == frame)\n",
    "                evictpos = max(D)\n",
    "                \n",
    "                if (seq_number % sampling_freq +1 == sampling_freq):\n",
    "                    #Y_OPT = populateData(LFUDict, LRUQ, C, D, CacheTS, CachePID)\n",
    "                    Y_OPT = populateData(LFUDict, LRUQ, C, D)\n",
    "                    lruPredict(C,LRUQ,Y_OPT)\n",
    "                    lfuPredict(C,LFUDict,Y_OPT)\n",
    "                \n",
    "                C[C.index(D[evictpos])] = block\n",
    "                LRUQ.remove(D[evictpos])\n",
    "                #del CacheTS [D[evictpos]]\n",
    "                #del CachePID [D[evictpos]]\n",
    "                del D[evictpos]\n",
    "            else:\n",
    "                C.append(block)\n",
    "                \n",
    "            if len(OPT[block]) is not 0:\n",
    "                D[OPT[block][0]] = block\n",
    "                OPT[block].popleft()\n",
    "            else:\n",
    "                D[maxpos] = block\n",
    "                maxpos -= 1\n",
    "            LRUQ.append(block)\n",
    "\n",
    "\n",
    "    hitrate = hit / (hit + miss)\n",
    "    #print(hitrate)\n",
    "    return hitrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 2. 3.]\n",
      "  [0. 1. 2.]\n",
      "  [2. 1. 4.]]]\n",
      "[[[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]]\n",
      "\n",
      " [[1.         2.         3.        ]\n",
      "  [0.         1.         2.        ]\n",
      "  [2.         1.         4.        ]]\n",
      "\n",
      " [[0.4472136  0.81649658 0.55708601]\n",
      "  [0.         0.40824829 0.37139068]\n",
      "  [0.89442719 0.40824829 0.74278135]]]\n"
     ]
    }
   ],
   "source": [
    "lfu = {2: 1, 3: 2, 4: 1}\n",
    "lru = [2, 3, 4]\n",
    "cache = [3, 2, 4]\n",
    "hidden = np.zeros((5, 3, 3))\n",
    "\n",
    "hidden = get_multiple_length_input(5, hidden, lfu, lru, cache, lambda x: x)\n",
    "print(hidden)\n",
    "hidden = get_multiple_length_input(5, hidden, lfu, lru, cache, normalize_columns)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
